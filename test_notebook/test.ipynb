{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd5ee95",
   "metadata": {},
   "source": [
    "# pytorch encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f11e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056838a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel_name = 'tf_efficientnet_b5_ap'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3de894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model ()..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/vivek/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Removing last two layers (global_pool & classifier).\n"
     ]
    }
   ],
   "source": [
    "print('Loading base model ()...'.format(basemodel_name), end='')\n",
    "basemodel = torch.hub.load('rwightman/gen-efficientnet-pytorch', basemodel_name, pretrained=True)\n",
    "print('Done.')\n",
    "\n",
    "# Remove last layer\n",
    "print('Removing last two layers (global_pool & classifier).')\n",
    "basemodel.global_pool = nn.Identity()\n",
    "basemodel.classifier = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fecc84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geffnet.gen_efficientnet.GenEfficientNet"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be3a10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 480, 640)\n",
    "features = [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b17fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in basemodel._modules.items():\n",
    "    if (k == 'blocks'):\n",
    "        for ki, vi in v._modules.items():\n",
    "            features.append(vi(features[-1]))\n",
    "    else:\n",
    "        features.append(v(features[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ddbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, backend):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.original_model = backend\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for k, v in self.original_model._modules.items():\n",
    "            if k == 'blocks':\n",
    "                for ki, vi in v._modules.items():\n",
    "                    features.append(vi(features[-1]))\n",
    "            else:\n",
    "                features.append(v(features[-1]))\n",
    "        return features\n",
    "\n",
    "\n",
    "def visualize_feature_maps(features, max_channels=8, figsize=(16, 4)):\n",
    "    \"\"\"\n",
    "    Visualizes the first few channels of each feature map in a row.\n",
    "    \n",
    "    Args:\n",
    "        features (list of Tensors): Feature maps from the encoder.\n",
    "        max_channels (int): Maximum number of channels to visualize per feature map.\n",
    "        figsize (tuple): Size of each subplot row.\n",
    "    \"\"\"\n",
    "    for i, feature in enumerate(features):\n",
    "        # Only visualize 4D feature maps (batch, channels, height, width)\n",
    "        if len(feature.shape) != 4:\n",
    "            continue\n",
    "        b, c, h, w = feature.shape\n",
    "        feature = feature[0]  # Visualize first sample in batch\n",
    "        \n",
    "        # Pick up to max_channels to display\n",
    "        n_channels = min(c, max_channels)\n",
    "        fig, axes = plt.subplots(1, n_channels, figsize=figsize)\n",
    "        fig.suptitle(f\"Layer {i} - Shape: {feature.shape}\")\n",
    "        \n",
    "        for j in range(n_channels):\n",
    "            ax = axes[j]\n",
    "            fmap = feature[j].detach().cpu().numpy()\n",
    "            ax.imshow(fmap, cmap='viridis')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"Channel {j}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd992c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483ee76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cfa95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 456, 456) \n",
    "l = encoder.forward(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4e4e50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Step 1: Encoder Definition ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, backend):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.original_model = backend\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for k, v in self.original_model._modules.items():\n",
    "            if k == 'blocks':\n",
    "                for ki, vi in v._modules.items():\n",
    "                    features.append(vi(features[-1]))\n",
    "            else:\n",
    "                features.append(v(features[-1]))\n",
    "        return features\n",
    "\n",
    "# ---------- Step 2: Feature Map Visualization ----------\n",
    "def visualize_feature_maps(features, max_channels=8, figsize=(16, 4)):\n",
    "    for i, feature in enumerate(features):\n",
    "        if len(feature.shape) != 4:\n",
    "            continue\n",
    "        b, c, h, w = feature.shape\n",
    "        feature = feature[0]  # First sample in batch\n",
    "\n",
    "        n_channels = min(c, max_channels)\n",
    "        fig, axes = plt.subplots(1, n_channels, figsize=figsize)\n",
    "        fig.suptitle(f\"Layer {i} - Shape: {feature.shape}\")\n",
    "\n",
    "        for j in range(n_channels):\n",
    "            ax = axes[j]\n",
    "            fmap = feature[j].detach().cpu().numpy()\n",
    "            ax.imshow(fmap, cmap='viridis')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"Channel {j}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ---------- Step 3: Load and Preprocess Image ----------\n",
    "def load_image(image_path, size=(224, 224)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),  # Converts HWC [0,255] â†’ CHW [0.0,1.0]\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension: [1, 3, H, W]\n",
    "    return tensor\n",
    "\n",
    "# ---------- Step 4: Run Everything ----------\n",
    "if __name__ == '__main__':\n",
    "    # Example usage\n",
    "    image_path = 'classroom__rgb_00283.jpg'  # Replace with your actual image path\n",
    "    input_tensor = load_image(image_path)  # torch.Size([1, 3, 224, 224])\n",
    "    \n",
    "    # # Replace this with your actual model\n",
    "    # backend_model = basemodel  # Ensure this has .conv_stem, .blocks etc.\n",
    "    # backend_model.eval()\n",
    "\n",
    "    # encoder = Encoder(backend_model)\n",
    "    # with torch.no_grad():\n",
    "    #     features = encoder(input_tensor)\n",
    "\n",
    "    # visualize_feature_maps(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e062c",
   "metadata": {},
   "source": [
    "# underatnding effecient net b5 for adabins tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de1b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 16:20:12.343772: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-05-24 16:20:12.343793: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-05-24 16:20:12.343798: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-05-24 16:20:12.343811: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-24 16:20:12.343822: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "base_model = EfficientNetB5(include_top=False, weights='imagenet', input_shape=(456, 456, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f9925c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Functional name=efficientnetb5, built=True>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dfba2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer\n",
      "rescaling\n",
      "normalization\n",
      "rescaling_1\n",
      "stem_conv_pad\n",
      "stem_conv\n",
      "stem_bn\n",
      "stem_activation\n",
      "block1a_dwconv\n",
      "block1a_bn\n",
      "block1a_activation\n",
      "block1a_se_squeeze\n",
      "block1a_se_reshape\n",
      "block1a_se_reduce\n",
      "block1a_se_expand\n",
      "block1a_se_excite\n",
      "block1a_project_conv\n",
      "block1a_project_bn\n",
      "block1b_dwconv\n",
      "block1b_bn\n",
      "block1b_activation\n",
      "block1b_se_squeeze\n",
      "block1b_se_reshape\n",
      "block1b_se_reduce\n",
      "block1b_se_expand\n",
      "block1b_se_excite\n",
      "block1b_project_conv\n",
      "block1b_project_bn\n",
      "block1b_drop\n",
      "block1b_add\n",
      "block1c_dwconv\n",
      "block1c_bn\n",
      "block1c_activation\n",
      "block1c_se_squeeze\n",
      "block1c_se_reshape\n",
      "block1c_se_reduce\n",
      "block1c_se_expand\n",
      "block1c_se_excite\n",
      "block1c_project_conv\n",
      "block1c_project_bn\n",
      "block1c_drop\n",
      "block1c_add\n",
      "block2a_expand_conv\n",
      "block2a_expand_bn\n",
      "block2a_expand_activation\n",
      "block2a_dwconv_pad\n",
      "block2a_dwconv\n",
      "block2a_bn\n",
      "block2a_activation\n",
      "block2a_se_squeeze\n",
      "block2a_se_reshape\n",
      "block2a_se_reduce\n",
      "block2a_se_expand\n",
      "block2a_se_excite\n",
      "block2a_project_conv\n",
      "block2a_project_bn\n",
      "block2b_expand_conv\n",
      "block2b_expand_bn\n",
      "block2b_expand_activation\n",
      "block2b_dwconv\n",
      "block2b_bn\n",
      "block2b_activation\n",
      "block2b_se_squeeze\n",
      "block2b_se_reshape\n",
      "block2b_se_reduce\n",
      "block2b_se_expand\n",
      "block2b_se_excite\n",
      "block2b_project_conv\n",
      "block2b_project_bn\n",
      "block2b_drop\n",
      "block2b_add\n",
      "block2c_expand_conv\n",
      "block2c_expand_bn\n",
      "block2c_expand_activation\n",
      "block2c_dwconv\n",
      "block2c_bn\n",
      "block2c_activation\n",
      "block2c_se_squeeze\n",
      "block2c_se_reshape\n",
      "block2c_se_reduce\n",
      "block2c_se_expand\n",
      "block2c_se_excite\n",
      "block2c_project_conv\n",
      "block2c_project_bn\n",
      "block2c_drop\n",
      "block2c_add\n",
      "block2d_expand_conv\n",
      "block2d_expand_bn\n",
      "block2d_expand_activation\n",
      "block2d_dwconv\n",
      "block2d_bn\n",
      "block2d_activation\n",
      "block2d_se_squeeze\n",
      "block2d_se_reshape\n",
      "block2d_se_reduce\n",
      "block2d_se_expand\n",
      "block2d_se_excite\n",
      "block2d_project_conv\n",
      "block2d_project_bn\n",
      "block2d_drop\n",
      "block2d_add\n",
      "block2e_expand_conv\n",
      "block2e_expand_bn\n",
      "block2e_expand_activation\n",
      "block2e_dwconv\n",
      "block2e_bn\n",
      "block2e_activation\n",
      "block2e_se_squeeze\n",
      "block2e_se_reshape\n",
      "block2e_se_reduce\n",
      "block2e_se_expand\n",
      "block2e_se_excite\n",
      "block2e_project_conv\n",
      "block2e_project_bn\n",
      "block2e_drop\n",
      "block2e_add\n",
      "block3a_expand_conv\n",
      "block3a_expand_bn\n",
      "block3a_expand_activation\n",
      "block3a_dwconv_pad\n",
      "block3a_dwconv\n",
      "block3a_bn\n",
      "block3a_activation\n",
      "block3a_se_squeeze\n",
      "block3a_se_reshape\n",
      "block3a_se_reduce\n",
      "block3a_se_expand\n",
      "block3a_se_excite\n",
      "block3a_project_conv\n",
      "block3a_project_bn\n",
      "block3b_expand_conv\n",
      "block3b_expand_bn\n",
      "block3b_expand_activation\n",
      "block3b_dwconv\n",
      "block3b_bn\n",
      "block3b_activation\n",
      "block3b_se_squeeze\n",
      "block3b_se_reshape\n",
      "block3b_se_reduce\n",
      "block3b_se_expand\n",
      "block3b_se_excite\n",
      "block3b_project_conv\n",
      "block3b_project_bn\n",
      "block3b_drop\n",
      "block3b_add\n",
      "block3c_expand_conv\n",
      "block3c_expand_bn\n",
      "block3c_expand_activation\n",
      "block3c_dwconv\n",
      "block3c_bn\n",
      "block3c_activation\n",
      "block3c_se_squeeze\n",
      "block3c_se_reshape\n",
      "block3c_se_reduce\n",
      "block3c_se_expand\n",
      "block3c_se_excite\n",
      "block3c_project_conv\n",
      "block3c_project_bn\n",
      "block3c_drop\n",
      "block3c_add\n",
      "block3d_expand_conv\n",
      "block3d_expand_bn\n",
      "block3d_expand_activation\n",
      "block3d_dwconv\n",
      "block3d_bn\n",
      "block3d_activation\n",
      "block3d_se_squeeze\n",
      "block3d_se_reshape\n",
      "block3d_se_reduce\n",
      "block3d_se_expand\n",
      "block3d_se_excite\n",
      "block3d_project_conv\n",
      "block3d_project_bn\n",
      "block3d_drop\n",
      "block3d_add\n",
      "block3e_expand_conv\n",
      "block3e_expand_bn\n",
      "block3e_expand_activation\n",
      "block3e_dwconv\n",
      "block3e_bn\n",
      "block3e_activation\n",
      "block3e_se_squeeze\n",
      "block3e_se_reshape\n",
      "block3e_se_reduce\n",
      "block3e_se_expand\n",
      "block3e_se_excite\n",
      "block3e_project_conv\n",
      "block3e_project_bn\n",
      "block3e_drop\n",
      "block3e_add\n",
      "block4a_expand_conv\n",
      "block4a_expand_bn\n",
      "block4a_expand_activation\n",
      "block4a_dwconv_pad\n",
      "block4a_dwconv\n",
      "block4a_bn\n",
      "block4a_activation\n",
      "block4a_se_squeeze\n",
      "block4a_se_reshape\n",
      "block4a_se_reduce\n",
      "block4a_se_expand\n",
      "block4a_se_excite\n",
      "block4a_project_conv\n",
      "block4a_project_bn\n",
      "block4b_expand_conv\n",
      "block4b_expand_bn\n",
      "block4b_expand_activation\n",
      "block4b_dwconv\n",
      "block4b_bn\n",
      "block4b_activation\n",
      "block4b_se_squeeze\n",
      "block4b_se_reshape\n",
      "block4b_se_reduce\n",
      "block4b_se_expand\n",
      "block4b_se_excite\n",
      "block4b_project_conv\n",
      "block4b_project_bn\n",
      "block4b_drop\n",
      "block4b_add\n",
      "block4c_expand_conv\n",
      "block4c_expand_bn\n",
      "block4c_expand_activation\n",
      "block4c_dwconv\n",
      "block4c_bn\n",
      "block4c_activation\n",
      "block4c_se_squeeze\n",
      "block4c_se_reshape\n",
      "block4c_se_reduce\n",
      "block4c_se_expand\n",
      "block4c_se_excite\n",
      "block4c_project_conv\n",
      "block4c_project_bn\n",
      "block4c_drop\n",
      "block4c_add\n",
      "block4d_expand_conv\n",
      "block4d_expand_bn\n",
      "block4d_expand_activation\n",
      "block4d_dwconv\n",
      "block4d_bn\n",
      "block4d_activation\n",
      "block4d_se_squeeze\n",
      "block4d_se_reshape\n",
      "block4d_se_reduce\n",
      "block4d_se_expand\n",
      "block4d_se_excite\n",
      "block4d_project_conv\n",
      "block4d_project_bn\n",
      "block4d_drop\n",
      "block4d_add\n",
      "block4e_expand_conv\n",
      "block4e_expand_bn\n",
      "block4e_expand_activation\n",
      "block4e_dwconv\n",
      "block4e_bn\n",
      "block4e_activation\n",
      "block4e_se_squeeze\n",
      "block4e_se_reshape\n",
      "block4e_se_reduce\n",
      "block4e_se_expand\n",
      "block4e_se_excite\n",
      "block4e_project_conv\n",
      "block4e_project_bn\n",
      "block4e_drop\n",
      "block4e_add\n",
      "block4f_expand_conv\n",
      "block4f_expand_bn\n",
      "block4f_expand_activation\n",
      "block4f_dwconv\n",
      "block4f_bn\n",
      "block4f_activation\n",
      "block4f_se_squeeze\n",
      "block4f_se_reshape\n",
      "block4f_se_reduce\n",
      "block4f_se_expand\n",
      "block4f_se_excite\n",
      "block4f_project_conv\n",
      "block4f_project_bn\n",
      "block4f_drop\n",
      "block4f_add\n",
      "block4g_expand_conv\n",
      "block4g_expand_bn\n",
      "block4g_expand_activation\n",
      "block4g_dwconv\n",
      "block4g_bn\n",
      "block4g_activation\n",
      "block4g_se_squeeze\n",
      "block4g_se_reshape\n",
      "block4g_se_reduce\n",
      "block4g_se_expand\n",
      "block4g_se_excite\n",
      "block4g_project_conv\n",
      "block4g_project_bn\n",
      "block4g_drop\n",
      "block4g_add\n",
      "block5a_expand_conv\n",
      "block5a_expand_bn\n",
      "block5a_expand_activation\n",
      "block5a_dwconv\n",
      "block5a_bn\n",
      "block5a_activation\n",
      "block5a_se_squeeze\n",
      "block5a_se_reshape\n",
      "block5a_se_reduce\n",
      "block5a_se_expand\n",
      "block5a_se_excite\n",
      "block5a_project_conv\n",
      "block5a_project_bn\n",
      "block5b_expand_conv\n",
      "block5b_expand_bn\n",
      "block5b_expand_activation\n",
      "block5b_dwconv\n",
      "block5b_bn\n",
      "block5b_activation\n",
      "block5b_se_squeeze\n",
      "block5b_se_reshape\n",
      "block5b_se_reduce\n",
      "block5b_se_expand\n",
      "block5b_se_excite\n",
      "block5b_project_conv\n",
      "block5b_project_bn\n",
      "block5b_drop\n",
      "block5b_add\n",
      "block5c_expand_conv\n",
      "block5c_expand_bn\n",
      "block5c_expand_activation\n",
      "block5c_dwconv\n",
      "block5c_bn\n",
      "block5c_activation\n",
      "block5c_se_squeeze\n",
      "block5c_se_reshape\n",
      "block5c_se_reduce\n",
      "block5c_se_expand\n",
      "block5c_se_excite\n",
      "block5c_project_conv\n",
      "block5c_project_bn\n",
      "block5c_drop\n",
      "block5c_add\n",
      "block5d_expand_conv\n",
      "block5d_expand_bn\n",
      "block5d_expand_activation\n",
      "block5d_dwconv\n",
      "block5d_bn\n",
      "block5d_activation\n",
      "block5d_se_squeeze\n",
      "block5d_se_reshape\n",
      "block5d_se_reduce\n",
      "block5d_se_expand\n",
      "block5d_se_excite\n",
      "block5d_project_conv\n",
      "block5d_project_bn\n",
      "block5d_drop\n",
      "block5d_add\n",
      "block5e_expand_conv\n",
      "block5e_expand_bn\n",
      "block5e_expand_activation\n",
      "block5e_dwconv\n",
      "block5e_bn\n",
      "block5e_activation\n",
      "block5e_se_squeeze\n",
      "block5e_se_reshape\n",
      "block5e_se_reduce\n",
      "block5e_se_expand\n",
      "block5e_se_excite\n",
      "block5e_project_conv\n",
      "block5e_project_bn\n",
      "block5e_drop\n",
      "block5e_add\n",
      "block5f_expand_conv\n",
      "block5f_expand_bn\n",
      "block5f_expand_activation\n",
      "block5f_dwconv\n",
      "block5f_bn\n",
      "block5f_activation\n",
      "block5f_se_squeeze\n",
      "block5f_se_reshape\n",
      "block5f_se_reduce\n",
      "block5f_se_expand\n",
      "block5f_se_excite\n",
      "block5f_project_conv\n",
      "block5f_project_bn\n",
      "block5f_drop\n",
      "block5f_add\n",
      "block5g_expand_conv\n",
      "block5g_expand_bn\n",
      "block5g_expand_activation\n",
      "block5g_dwconv\n",
      "block5g_bn\n",
      "block5g_activation\n",
      "block5g_se_squeeze\n",
      "block5g_se_reshape\n",
      "block5g_se_reduce\n",
      "block5g_se_expand\n",
      "block5g_se_excite\n",
      "block5g_project_conv\n",
      "block5g_project_bn\n",
      "block5g_drop\n",
      "block5g_add\n",
      "block6a_expand_conv\n",
      "block6a_expand_bn\n",
      "block6a_expand_activation\n",
      "block6a_dwconv_pad\n",
      "block6a_dwconv\n",
      "block6a_bn\n",
      "block6a_activation\n",
      "block6a_se_squeeze\n",
      "block6a_se_reshape\n",
      "block6a_se_reduce\n",
      "block6a_se_expand\n",
      "block6a_se_excite\n",
      "block6a_project_conv\n",
      "block6a_project_bn\n",
      "block6b_expand_conv\n",
      "block6b_expand_bn\n",
      "block6b_expand_activation\n",
      "block6b_dwconv\n",
      "block6b_bn\n",
      "block6b_activation\n",
      "block6b_se_squeeze\n",
      "block6b_se_reshape\n",
      "block6b_se_reduce\n",
      "block6b_se_expand\n",
      "block6b_se_excite\n",
      "block6b_project_conv\n",
      "block6b_project_bn\n",
      "block6b_drop\n",
      "block6b_add\n",
      "block6c_expand_conv\n",
      "block6c_expand_bn\n",
      "block6c_expand_activation\n",
      "block6c_dwconv\n",
      "block6c_bn\n",
      "block6c_activation\n",
      "block6c_se_squeeze\n",
      "block6c_se_reshape\n",
      "block6c_se_reduce\n",
      "block6c_se_expand\n",
      "block6c_se_excite\n",
      "block6c_project_conv\n",
      "block6c_project_bn\n",
      "block6c_drop\n",
      "block6c_add\n",
      "block6d_expand_conv\n",
      "block6d_expand_bn\n",
      "block6d_expand_activation\n",
      "block6d_dwconv\n",
      "block6d_bn\n",
      "block6d_activation\n",
      "block6d_se_squeeze\n",
      "block6d_se_reshape\n",
      "block6d_se_reduce\n",
      "block6d_se_expand\n",
      "block6d_se_excite\n",
      "block6d_project_conv\n",
      "block6d_project_bn\n",
      "block6d_drop\n",
      "block6d_add\n",
      "block6e_expand_conv\n",
      "block6e_expand_bn\n",
      "block6e_expand_activation\n",
      "block6e_dwconv\n",
      "block6e_bn\n",
      "block6e_activation\n",
      "block6e_se_squeeze\n",
      "block6e_se_reshape\n",
      "block6e_se_reduce\n",
      "block6e_se_expand\n",
      "block6e_se_excite\n",
      "block6e_project_conv\n",
      "block6e_project_bn\n",
      "block6e_drop\n",
      "block6e_add\n",
      "block6f_expand_conv\n",
      "block6f_expand_bn\n",
      "block6f_expand_activation\n",
      "block6f_dwconv\n",
      "block6f_bn\n",
      "block6f_activation\n",
      "block6f_se_squeeze\n",
      "block6f_se_reshape\n",
      "block6f_se_reduce\n",
      "block6f_se_expand\n",
      "block6f_se_excite\n",
      "block6f_project_conv\n",
      "block6f_project_bn\n",
      "block6f_drop\n",
      "block6f_add\n",
      "block6g_expand_conv\n",
      "block6g_expand_bn\n",
      "block6g_expand_activation\n",
      "block6g_dwconv\n",
      "block6g_bn\n",
      "block6g_activation\n",
      "block6g_se_squeeze\n",
      "block6g_se_reshape\n",
      "block6g_se_reduce\n",
      "block6g_se_expand\n",
      "block6g_se_excite\n",
      "block6g_project_conv\n",
      "block6g_project_bn\n",
      "block6g_drop\n",
      "block6g_add\n",
      "block6h_expand_conv\n",
      "block6h_expand_bn\n",
      "block6h_expand_activation\n",
      "block6h_dwconv\n",
      "block6h_bn\n",
      "block6h_activation\n",
      "block6h_se_squeeze\n",
      "block6h_se_reshape\n",
      "block6h_se_reduce\n",
      "block6h_se_expand\n",
      "block6h_se_excite\n",
      "block6h_project_conv\n",
      "block6h_project_bn\n",
      "block6h_drop\n",
      "block6h_add\n",
      "block6i_expand_conv\n",
      "block6i_expand_bn\n",
      "block6i_expand_activation\n",
      "block6i_dwconv\n",
      "block6i_bn\n",
      "block6i_activation\n",
      "block6i_se_squeeze\n",
      "block6i_se_reshape\n",
      "block6i_se_reduce\n",
      "block6i_se_expand\n",
      "block6i_se_excite\n",
      "block6i_project_conv\n",
      "block6i_project_bn\n",
      "block6i_drop\n",
      "block6i_add\n",
      "block7a_expand_conv\n",
      "block7a_expand_bn\n",
      "block7a_expand_activation\n",
      "block7a_dwconv\n",
      "block7a_bn\n",
      "block7a_activation\n",
      "block7a_se_squeeze\n",
      "block7a_se_reshape\n",
      "block7a_se_reduce\n",
      "block7a_se_expand\n",
      "block7a_se_excite\n",
      "block7a_project_conv\n",
      "block7a_project_bn\n",
      "block7b_expand_conv\n",
      "block7b_expand_bn\n",
      "block7b_expand_activation\n",
      "block7b_dwconv\n",
      "block7b_bn\n",
      "block7b_activation\n",
      "block7b_se_squeeze\n",
      "block7b_se_reshape\n",
      "block7b_se_reduce\n",
      "block7b_se_expand\n",
      "block7b_se_excite\n",
      "block7b_project_conv\n",
      "block7b_project_bn\n",
      "block7b_drop\n",
      "block7b_add\n",
      "block7c_expand_conv\n",
      "block7c_expand_bn\n",
      "block7c_expand_activation\n",
      "block7c_dwconv\n",
      "block7c_bn\n",
      "block7c_activation\n",
      "block7c_se_squeeze\n",
      "block7c_se_reshape\n",
      "block7c_se_reduce\n",
      "block7c_se_expand\n",
      "block7c_se_excite\n",
      "block7c_project_conv\n",
      "block7c_project_bn\n",
      "block7c_drop\n",
      "block7c_add\n",
      "top_conv\n",
      "top_bn\n",
      "top_activation\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for layer in base_model.layers:\n",
    "    print(layer.name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3537c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 456, 456, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB5, preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ---------- Step 1: Encoder Definition ----------\n",
    "import inspect\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, base_model):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        features = [x]\n",
    "        for layer in self.base_model.layers:\n",
    "            sig = inspect.signature(layer.call)\n",
    "            if 'training' in sig.parameters:\n",
    "                x = layer(x, training=training)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            features.append(x)\n",
    "        return features\n",
    "# ---------- Step 2: Load and Preprocess Image ----------\n",
    "def load_image(image_path, size=(456, 456)):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize(size)\n",
    "    image = np.array(image).astype(np.float32)\n",
    "    image = preprocess_input(image)  # For EfficientNet preprocessing\n",
    "    image = np.expand_dims(image, axis=0)  # [1, H, W, 3]\n",
    "    return image\n",
    "\n",
    "# ---------- Step 3: Visualize Feature Maps ----------\n",
    "def visualize_feature_maps(features, max_channels=8, figsize=(16, 4)):\n",
    "    for i, feature in enumerate(features):\n",
    "        if len(feature.shape) != 4:\n",
    "            continue\n",
    "        fmap = feature[0]  # First image in batch\n",
    "        h, w, c = fmap.shape[0], fmap.shape[1], fmap.shape[2]\n",
    "        n_channels = min(c, max_channels)\n",
    "\n",
    "        fig, axes = plt.subplots(1, n_channels, figsize=figsize)\n",
    "        fig.suptitle(f\"Layer {i} - Shape: {fmap.shape}\")\n",
    "\n",
    "        for j in range(n_channels):\n",
    "            ax = axes[j]\n",
    "            channel_image = fmap[:, :, j]\n",
    "            ax.imshow(channel_image, cmap='viridis')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"Ch {j}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ---------- Step 4: Main Run ----------\n",
    "if __name__ == '__main__':\n",
    "    image_path = 'classroom__rgb_00283.jpg'  # Replace with your actual image path\n",
    "    input_image = load_image(image_path) # (1, 456, 456, 3)\n",
    "    \n",
    "    # Load base model without top layers\n",
    "    base_model = EfficientNetB5(include_top=False, weights='imagenet', input_shape=(456, 456, 3))\n",
    "    \n",
    "    # encoder = Encoder(base_model)\n",
    "\n",
    "    # # Forward pass\n",
    "    # features = encoder(input_image, training=False)\n",
    "\n",
    "    # # Visualize\n",
    "    # visualize_feature_maps(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054dbf37",
   "metadata": {},
   "source": [
    "# tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3849db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1da940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 12:44:03.156137: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-05-21 12:44:03.156167: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-05-21 12:44:03.156174: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-05-21 12:44:03.156190: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-21 12:44:03.156203: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "input_shape=(480, 640, 3)\n",
    "base_model = tf.keras.applications.EfficientNetB5(include_top=False, weights='imagenet', input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf4239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224073d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "too many positional arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m features \u001b[38;5;241m=\u001b[39m [x]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/inspect.py:3186\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/inspect.py:3107\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3105\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(parameters)\n\u001b[1;32m   3106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m-> 3107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   3110\u001b[0m         \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m         \u001b[38;5;66;03m# argument\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: too many positional arguments"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([2, 480, 640, 3])  # batch of 2 RGB images, height=480, width=640\n",
    "features = [x]\n",
    "for layer in base_model.layers:\n",
    "    x = layer(x)\n",
    "    features.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9385051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.src.models.functional.Functional"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0abfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_block0 shape: (2, 120, 160, 144)\n",
      "x_block1 shape: (2, 60, 80, 240)\n",
      "x_block2 shape: (2, 30, 40, 384)\n",
      "x_block3 shape: (2, 15, 20, 1056)\n",
      "x_block4 shape: (2, 15, 20, 2048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Encoder using EfficientNetB5\n",
    "# ----------------------\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, input_shape=(480, 640, 3)):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Load pretrained EfficientNetB5 without classifier (top)\n",
    "        base_model = EfficientNetB5(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "        # Specify skip connection layers\n",
    "        skip_layer_names = [\n",
    "            'block2a_activation',  # x_block0\n",
    "            'block3a_activation',  # x_block1\n",
    "            'block4a_activation',  # x_block2\n",
    "            'block6a_activation',  # x_block3\n",
    "            'top_activation'       # x_block4 (bottleneck input)\n",
    "        ]\n",
    "\n",
    "        # Extract outputs from these layers\n",
    "        skip_outputs = [base_model.get_layer(name).output for name in skip_layer_names]\n",
    "\n",
    "        # Create the intermediate feature model\n",
    "        self.feature_extractor = Model(inputs=base_model.input, outputs=skip_outputs)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.feature_extractor(x)  # returns [x_block0, x_block1, x_block2, x_block3, x_block4]\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Testing the Encoder\n",
    "# ----------------------\n",
    "if __name__ == '__main__':\n",
    "    # Create encoder\n",
    "    encoder = Encoder(input_shape=(480, 640, 3))\n",
    "\n",
    "    # Example input (batch of 2 images)\n",
    "    x = tf.random.normal([2, 480, 640, 3])\n",
    "\n",
    "    # Get intermediate features for skip connections\n",
    "    features = encoder(x)\n",
    "\n",
    "    for i, f in enumerate(features):\n",
    "        print(f\"x_block{i} shape:\", f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9ac4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://i.imgur.com/Bvro0YD.png\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://i.imgur.com/Bvro0YD.png: 429 -- Unknown Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/file_utils.py:291\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/urllib/request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    562\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    495\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 429: Unknown Error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Load an image (you can replace this with any RGB image path)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melephant.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://i.imgur.com/Bvro0YD.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m img \u001b[38;5;241m=\u001b[39m keras_image\u001b[38;5;241m.\u001b[39mload_img(img_path)\n\u001b[1;32m     62\u001b[0m img \u001b[38;5;241m=\u001b[39m keras_image\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/file_utils.py:293\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[1;32m    291\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39merrno, e\u001b[38;5;241m.\u001b[39mreason))\n",
      "\u001b[0;31mException\u001b[0m: URL fetch failure on https://i.imgur.com/Bvro0YD.png: 429 -- Unknown Error"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import EfficientNetB5, efficientnet\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, input_shape=(480, 640, 3)):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Load pretrained EfficientNetB5 (no top classification layer)\n",
    "        base_model = EfficientNetB5(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "        # Layer names to tap for skip connections\n",
    "        self.skip_layer_names = [\n",
    "            'block2a_activation',  # x_block0\n",
    "            'block3a_activation',  # x_block1\n",
    "            'block4a_activation',  # x_block2\n",
    "            'block6a_activation',  # x_block3\n",
    "            'top_activation'       # x_block4 (deepest)\n",
    "        ]\n",
    "\n",
    "        # Extract those outputs\n",
    "        skip_outputs = [base_model.get_layer(name).output for name in self.skip_layer_names]\n",
    "\n",
    "        # Wrap into a model\n",
    "        self.feature_extractor = Model(inputs=base_model.input, outputs=skip_outputs)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "    def extract_features_from_image(self, img, resize_shape=(480, 640)):\n",
    "        \"\"\"\n",
    "        img: input image as numpy array [H, W, C] in RGB format\n",
    "        resize_shape: target size (H, W)\n",
    "        \"\"\"\n",
    "        # Resize image\n",
    "        img_resized = cv2.resize(img, resize_shape[::-1])  # cv2 uses (W, H)\n",
    "\n",
    "        # Convert to float32 & preprocess\n",
    "        img_preprocessed = efficientnet.preprocess_input(img_resized.astype(np.float32))\n",
    "\n",
    "        # Add batch dimension\n",
    "        img_tensor = tf.expand_dims(img_preprocessed, axis=0)\n",
    "\n",
    "        # Extract features\n",
    "        features = self(img_tensor)\n",
    "        return features\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Test the Encoder with an image\n",
    "# ----------------------\n",
    "if __name__ == '__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load an image (you can replace this with any RGB image path)\n",
    "    img = cv2.imrea\n",
    "    img = keras_image.img_to_array(img)\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Encoder()\n",
    "\n",
    "    # Extract intermediate features\n",
    "    features = encoder.extract_features_from_image(img)\n",
    "\n",
    "    # Print and visualize shapes\n",
    "    for i, f in enumerate(features):\n",
    "        print(f\"x_block{i} shape: {f.shape}\")\n",
    "\n",
    "        # Optional: visualize the first channel\n",
    "        fmap = f[0, :, :, 0].numpy()\n",
    "        plt.figure()\n",
    "        plt.imshow(fmap, cmap='viridis')\n",
    "        plt.title(f\"Feature Map x_block{i} (channel 0)\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc5e7d",
   "metadata": {},
   "source": [
    "# TransformerEncoderLayer - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f33336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoderLayer(layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, ff_dim=1024, dropout_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        # Feedforward network (FFN)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),  # Hidden layer\n",
    "            layers.Dense(embedding_dim)  # Output layer (same dimension as input)\n",
    "        ])\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "            # Self-Attention Layer\n",
    "            attn_output = self.attn(inputs, inputs)\n",
    "            # Residual Connection + Layer Normalization\n",
    "            attn_output = self.dropout1(attn_output, training=training)\n",
    "            out1 = self.layernorm1(inputs + attn_output)\n",
    "            \n",
    "            # Feedforward Network\n",
    "            ffn_output = self.ffn(out1)\n",
    "            # Residual Connection + Layer Normalization\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            out2 = self.layernorm2(out1 + ffn_output)\n",
    "            \n",
    "            return out2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710109e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 132, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage\n",
    "embedding_dim = 128\n",
    "num_heads = 4\n",
    "dim_feedforward = 1024\n",
    "num_layers = 6  # Number of Encoder Layers\n",
    "\n",
    "# Create a transformer encoder layer\n",
    "encoder_layer = TransformerEncoderLayer(embedding_dim, num_heads, ff_dim=dim_feedforward)\n",
    "\n",
    "# Example input (batch_size, sequence_length, embedding_dim)\n",
    "x = tf.random.uniform((16, 132, embedding_dim))  # (Batch size, Sequence length, Embedding dimension)\n",
    "\n",
    "# Pass input through the Transformer Encoder Layer\n",
    "output = encoder_layer(x)\n",
    "\n",
    "print(output.shape)  # Output shape should be (batch_size, seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e503a0f",
   "metadata": {},
   "source": [
    "# transformerEncoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating the transformerEncoder from TransformerEncoderBlock\n",
    "transformer_layers = [\n",
    "            TransformerEncoderLayer(embedding_dim=embedding_dim, num_heads=num_heads, ff_dim=1024)\n",
    "            for _ in range(num_layers)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3ffa4",
   "metadata": {},
   "source": [
    "# patch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0900909",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "embedding_dim = 512\n",
    "\n",
    "\n",
    "embedding_convPxP = layers.Conv2D(\n",
    "            filters=embedding_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding='valid',\n",
    "            use_bias=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285355ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = tf.keras.Input(shape=(64, 64, 3))  # (H, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4311bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = embedding_convPxP(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa2dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 4, 4, 512)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformerEncoder(tf.keras.Model):\n",
    "    def __init__(self, in_channels, patch_size=10, embedding_dim=128, num_heads=4, num_layers=4):\n",
    "        super(PatchTransformerEncoder, self).__init__()\n",
    "\n",
    "        #generating the traininable positional encodings\n",
    "        self.positional_encodings = self.add_weight(\n",
    "            shape=(16, embedding_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be020ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PatchTransformerEncoder(in_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ce333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasVariable shape=(16, 128), dtype=float32, path=patch_transformer_encoder/variable>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.positional_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29103038",
   "metadata": {},
   "source": [
    "# implenmenting the pathtransformerEncoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3b3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
