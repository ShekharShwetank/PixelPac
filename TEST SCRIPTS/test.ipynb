{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input, models\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb66ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteEncoderDecoder:\n",
    "    def __init__(self, input_shape=(240, 240, 3)):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def depthwise_separable_conv(self, x, out_channels=None, strides=1, name=None):\n",
    "        x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        if out_channels is not None:\n",
    "            x = layers.Conv2D(out_channels, kernel_size=1, padding='same')(x)\n",
    "        x = layers.DepthwiseConv2D(kernel_size=3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU(name=name)(x)\n",
    "        return x\n",
    "\n",
    "    def lightweight_block(self, x):\n",
    "        x = layers.DepthwiseConv2D(kernel_size=3, padding='same')(x)\n",
    "        x = layers.Conv2D(8, kernel_size=1, padding='same')(x)\n",
    "        x = layers.Conv2D(8, kernel_size=3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU(name=\"A\")(x)\n",
    "        return x\n",
    "\n",
    "    def downsample_block(self, x, double_channels=False, name=None):\n",
    "        channels = x.shape[-1]\n",
    "        x = layers.DepthwiseConv2D(kernel_size=3, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        out_channels = channels * 2 if double_channels else channels\n",
    "        x = layers.Conv2D(out_channels, kernel_size=1, padding='same')(x)\n",
    "        x = layers.DepthwiseConv2D(kernel_size=3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU(name=name)(x)\n",
    "        return x\n",
    "\n",
    "    def fuse_block(self, low_res, high_res, out_channels, name=None, use_conv=True):\n",
    "        x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(low_res)\n",
    "        x = layers.Concatenate()([x, high_res])\n",
    "        if use_conv:\n",
    "            x = layers.Conv2D(out_channels, kernel_size=3, padding='same')(x)\n",
    "        x = layers.DepthwiseConv2D(kernel_size=3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Conv2D(out_channels, kernel_size=1, padding='same')(x)\n",
    "        x = layers.DepthwiseConv2D(kernel_size=3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU(name=name)(x)\n",
    "        return x\n",
    "\n",
    "    def build_model(self):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.lightweight_block(inputs)                              # 240x240x8\n",
    "        x2 = self.downsample_block(x1, double_channels=True, name=\"B\")  # 120x120x16\n",
    "        x3 = self.downsample_block(x2, double_channels=True, name=\"C\")  # 60x60x32\n",
    "        x4 = self.downsample_block(x3, double_channels=True, name=\"D\")  # 30x30x64\n",
    "        x5 = self.downsample_block(x4, double_channels=True, name=\"E\")  # 15x15x128\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.fuse_block(x5, x4, 96, name=\"J\")\n",
    "        d3 = self.fuse_block(d4, x3, 40, name=\"K\", use_conv=False)\n",
    "        d2 = self.fuse_block(d3, x2, 58, name=\"L\")\n",
    "        d1 = self.fuse_block(d2, x1, 64, name=\"out\", use_conv=False)\n",
    "\n",
    "        return Model(inputs, d1, name=\"LiteEncoderDecoder\")\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "\n",
    "        #multi head attention block form keras\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim // num_heads)\n",
    "        #feedforward layer\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(feed_forward_dim, activation='relu'),\n",
    "            layers.Dense(embedding_dim)\n",
    "        ])\n",
    "        #normalization and dropout\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        #forward pass for TrnsformerEncodeblock\n",
    "        attn_output = self.att(x, x)\n",
    "        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
    "    \n",
    "\n",
    "\n",
    "class PatchTransformerEncoder(tf.keras.Model):\n",
    "    def __init__(self, in_channels, patch_size, embedding_dim, num_heads, num_layers):\n",
    "        super(PatchTransformerEncoder, self).__init__()\n",
    "\n",
    "        print(f\"in_channels : {in_channels} \\npatch_size : {patch_size} \\nembedding_dim : {embedding_dim} \\nnum_heads : {num_heads} \\nnum_layers : {num_layers}\")\n",
    "\n",
    "        #creating the transformerEncoder from TransformerEncoderBlock\n",
    "        self.transformer_layers = [\n",
    "            TransformerEncoderBlock(embedding_dim=embedding_dim, num_heads=num_heads, feed_forward_dim=512)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.embedding_convPxP = layers.Conv2D(\n",
    "            filters=embedding_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding='valid',\n",
    "            use_bias=False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #generating the traininable positional encodings\n",
    "        self.positional_encodings = self.add_weight(\n",
    "            shape=(225, embedding_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "        # x: [batch,H, W, C] expected in TF\n",
    "        x = self.embedding_convPxP(x)  # [batch, H', W', embedding_dim] -> tf.Tensor([ 1 12 16 64], shape=(4,), dtype=int32)\n",
    "        batch_size, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "\n",
    "        x = tf.reshape(x, [batch_size, h * w, c])  # [batch, tokens, embedding_dim]\n",
    "        positional = self.positional_encodings # generating the positional encoding\n",
    "        \n",
    "        \n",
    "\n",
    "        x += positional #concating path embeding  and positional embedding\n",
    "        \n",
    "        x = tf.transpose(x, perm=[1,0,2]) #transpose to form -> S,N,E i.e sequence lenght , batch size and embedding dim\n",
    "        \n",
    "        \n",
    "        \n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        \n",
    "        return x  # [batch, tokens, embedding_dim]\n",
    "        \n",
    "class PixelWiseDotProduct(tf.keras.Model):\n",
    "    def call(self, X):\n",
    "        x, K = X[0], X[1]\n",
    "        batch_size, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        _, cout, ck = tf.shape(K)[0], tf.shape(K)[1], tf.shape(K)[2]\n",
    "\n",
    "       \n",
    "        # Do this:\n",
    "        x_flat = tf.reshape(x, [batch_size, h * w, c])  # [batch, HW, C]\n",
    "        y = tf.matmul(x_flat, K, transpose_b=True)      # [batch, HW, out_channels]\n",
    "\n",
    "        y = tf.transpose(y, [0, 2, 1])        # [batch, out_channels, HW]\n",
    "        y = tf.reshape(y, [batch_size, tf.shape(K)[1], h, w])  # [batch, out_channels, H, W]\n",
    "\n",
    "        return y\n",
    "\n",
    "# from layers import PatchTransformerEncoder, PixelWiseDotProduct\n",
    "\n",
    "# Define mViT and build_mViT_model as you already have\n",
    "def export_full_tflite_model():\n",
    "    # Build model\n",
    "    model = build_mViT_model(\n",
    "        input_shape=(240, 240, 3),\n",
    "        in_channels=3,\n",
    "        n_query_channels=128,\n",
    "        patch_size=20,\n",
    "        dim_out=256,\n",
    "        embedding_dim=64,\n",
    "        num_heads=2,\n",
    "        norm='linear'\n",
    "    )\n",
    "\n",
    "    # Run a forward pass to build the model\n",
    "    dummy_input = tf.random.normal([1, 240, 240, 3])\n",
    "    _ = model(dummy_input)\n",
    "\n",
    "    # Export as a SavedModel for TFLite\n",
    "    saved_model_dir = \"full_mvit_savedmodel\"\n",
    "    model.export(saved_model_dir)\n",
    "    print(f\"âœ… SavedModel exported to: {saved_model_dir}\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter.experimental_enable_resource_variables = True\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save TFLite model\n",
    "    tflite_model_path = \"full_mvit_model.tflite\"\n",
    "    with open(tflite_model_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    print(f\"âœ… Full mViT model converted to TFLite and saved as: {tflite_model_path}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class mViT(tf.keras.Model):\n",
    "    def __init__(self, in_channels, n_query_channels=64, patch_size=20, dim_out=128, embedding_dim=64, num_heads=2, norm='linear'):\n",
    "        super(mViT, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.n_query_channels = n_query_channels\n",
    "\n",
    "        self.patch_transformer = PatchTransformerEncoder(\n",
    "            in_channels=in_channels,\n",
    "            patch_size=patch_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=2\n",
    "        )\n",
    "\n",
    "        self.dot_product_layer = PixelWiseDotProduct()\n",
    "\n",
    "        self.conv3x3 = layers.Conv2D(\n",
    "            filters=embedding_dim,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "\n",
    "        self.regressor = tf.keras.Sequential([\n",
    "            layers.Dense(256),\n",
    "            layers.LeakyReLU(),\n",
    "            layers.Dense(256),\n",
    "            layers.LeakyReLU(),\n",
    "            layers.Dense(dim_out)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x: [batch, height, width, channels]\n",
    "        tgt = self.patch_transformer(tf.identity(x))  # [token, batch, embedding_dim]\n",
    "        \n",
    "        x = self.conv3x3(x)  # [batch, height, width, embedding_dim]\n",
    "        \n",
    "        regression_head = tgt[0, :, :]  # [batch, embedding_dim]\n",
    "        \n",
    "        queries = tgt[ 1:self.n_query_channels + 1, :]  # [batch, n_query_channels, embedding_dim]\n",
    "        \n",
    "        queries = tf.transpose(queries, perm=[1,0,2])\n",
    "        \n",
    "        # Pixel-wise dot product: x is [batch, h, w, embedding_dim], queries is [batch, n_query_channels, embedding_dim]\n",
    "        range_attention_maps = self.dot_product_layer((x, queries))  # [batch, n_query_channels, h, w]\n",
    "        \n",
    "\n",
    "        y = self.regressor(regression_head)  # [batch, dim_out]\n",
    "        \n",
    "        \n",
    "        if self.norm == 'linear':\n",
    "            y = tf.nn.relu(y)\n",
    "            eps = 0.1\n",
    "            y = y + eps\n",
    "        elif self.norm == 'softmax':\n",
    "            return tf.nn.softmax(y, axis=1), range_attention_maps\n",
    "        else:\n",
    "            y = tf.nn.sigmoid(y)\n",
    "\n",
    "        y = y / tf.reduce_sum(y, axis=1, keepdims=True)\n",
    "        return y, range_attention_maps\n",
    "\n",
    "\n",
    "\n",
    "# from miniVit import mViT\n",
    "# from encoder_decoder import LiteEncoderDecoder\n",
    "\n",
    "\n",
    "class UnetAdaptiveBins(tf.keras.Model):\n",
    "    def __init__(self, backend, n_bins = 256, min_val = 0.1, max_val = 80, norm = \"linear\"):\n",
    "        super(UnetAdaptiveBins, self).__init__()\n",
    "\n",
    "        self.num_classes = n_bins\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "        self.encoder_decoder = LiteEncoderDecoder()\n",
    "        self.adaptive_bins_layers= mViT(\n",
    "                                    in_channels=64,\n",
    "                                    n_query_channels=64,\n",
    "                                    patch_size=16,\n",
    "                                    dim_out=n_bins,\n",
    "                                    embedding_dim=64,\n",
    "                                    num_heads=4,\n",
    "                                    norm='linear'\n",
    "                                        )\n",
    "        self.conv_out= models.Sequential([\n",
    "                                    layers.Conv2D(filters=n_bins, kernel_size=1, strides=1, padding='valid'),\n",
    "                                    layers.Softmax(axis=3)  # Softmax applied on the channel dimension \n",
    "                                ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        encoder_decoder_out = self.encoder_decoder.model(x)\n",
    "       \n",
    "        bin_widths_normed, range_attention_maps = self.adaptive_bins_layers(encoder_decoder_out)\n",
    "        \n",
    "        range_attention_maps = tf.transpose(range_attention_maps, [0, 2, 3, 1])  # (1, 240, 240, 64)\n",
    "        out = self.conv_out(range_attention_maps)\n",
    "        bin_widths = (self.max_val - self.min_val) * bin_widths_normed\n",
    "\n",
    "\n",
    "        min_vals = tf.fill([tf.shape(bin_widths)[0], 1], self.min_val)\n",
    "        bin_widths = tf.concat([min_vals, bin_widths], axis=1)\n",
    "        bin_edges = tf.math.cumsum(bin_widths, axis=1)\n",
    "        \n",
    "        centers = 0.5 * (bin_edges[:, :-1] + bin_edges[:, 1:])  # shape: [batch, n_bins]\n",
    "        centers = tf.reshape(centers, [tf.shape(centers)[0], tf.shape(centers)[1], 1, 1])\n",
    "    \n",
    "        out_permuted = tf.transpose(out, [0, 3, 1, 2])  # [B, n_bins, H, W]\n",
    "\n",
    "        pred = tf.reduce_sum(out_permuted * centers, axis=1, keepdims=True)  # [B, 1, H, W]\n",
    "        \n",
    "        return bin_edges, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341fa48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def load_unet_from_checkpoint(checkpoint_dir=\"./checkpoints\", num_bins=100, learning_rate=3e-4):\n",
    "    print(f\"[INFO] Loading checkpoint from {checkpoint_dir}\")\n",
    "    model = UnetAdaptiveBins(num_bins)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    if manager.latest_checkpoint:\n",
    "        checkpoint.restore(manager.latest_checkpoint).expect_partial()\n",
    "        print(f\"[INFO] Restored from checkpoint: {manager.latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"[WARNING] No checkpoint found. Starting fresh.\")\n",
    "\n",
    "    return model, optimizer, manager\n",
    "\n",
    "def predict_depth(model, img):\n",
    "    img_resized = tf.image.resize(img, [240, 240], method='bilinear')\n",
    "    img_input = tf.expand_dims(img_resized, axis=0)\n",
    "    _, pred_depth = model(img_input, training=False)\n",
    "\n",
    "    depth = tf.squeeze(pred_depth, axis=0)  # [240, 240]\n",
    "    depth = tf.squeeze(depth, axis=0) if len(depth.shape) == 3 else depth\n",
    "    depth_np = depth.numpy()\n",
    "\n",
    "    depth_norm = cv2.normalize(depth_np, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    depth_colored = cv2.applyColorMap(depth_norm.astype(np.uint8), cv2.COLORMAP_PLASMA)\n",
    "\n",
    "    return depth_colored\n",
    "\n",
    "def process_directory_to_video(model, image_dir, output_video_path, fps=10):\n",
    "    image_files = sorted([\n",
    "        os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ])\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"[ERROR] No image files found in {image_dir}\")\n",
    "        return\n",
    "\n",
    "    frame_width = 240 * 2\n",
    "    frame_height = 240\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(f\"[INFO] Writing video to: {output_video_path}\")\n",
    "\n",
    "    for img_path in image_files:\n",
    "        img_bgr = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        img_rgb = tf.convert_to_tensor(img_rgb, dtype=tf.float32) / 255.0\n",
    "\n",
    "        depth_map = predict_depth(model, img_rgb)\n",
    "        rgb_resized = cv2.resize(img_bgr, (240, 240))\n",
    "        combined = np.hstack((rgb_resized, depth_map))\n",
    "\n",
    "        out.write(combined)\n",
    "\n",
    "    out.release()\n",
    "    print(\"[INFO] Video generation complete.\")\n",
    "\n",
    "def run_camera(model):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"[ERROR] Cannot open webcam.\")\n",
    "        return\n",
    "\n",
    "    print(\"[INFO] Press 'q' to exit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img_tf = tf.convert_to_tensor(frame, dtype=tf.float32) / 255.0\n",
    "        depth_map = predict_depth(model, img_tf)\n",
    "        frame_resized = cv2.resize(frame, (240, 240))\n",
    "        combined = np.hstack((frame_resized, depth_map))\n",
    "\n",
    "        cv2.imshow(\"Camera Feed (Left) | Depth Map (Right)\", combined)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Polymorphic main function\n",
    "def main(image_dir=None, chekcpointdir =\"./checkpoints\" ,output_video_path=\"output_depth_video.mp4\"):\n",
    "    model, _, _ = load_unet_from_checkpoint(checkpoint_dir= chekcpointdir)\n",
    "\n",
    "    if image_dir:\n",
    "        process_directory_to_video(model, image_dir, output_video_path)\n",
    "    else:\n",
    "        run_camera(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23706ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading checkpoint from versin 12/checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 19:42:51.813290: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-06-07 19:42:51.813312: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-06-07 19:42:51.813315: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-06-07 19:42:51.813332: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-07 19:42:51.813347: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels : 64 \n",
      "patch_size : 16 \n",
      "embedding_dim : 64 \n",
      "num_heads : 4 \n",
      "num_layers : 2\n",
      "[INFO] Restored from checkpoint: versin 12/checkpoints/ckpt-75\n",
      "[INFO] Writing video to: output_depth_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mlops/lib/python3.10/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (225, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Video generation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_dir = \"test_data\"\n",
    "checkpoint = \"versin 12/checkpoints\"\n",
    "main(chekcpointdir=checkpoint)  # Process directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79e3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aaacf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457295b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
